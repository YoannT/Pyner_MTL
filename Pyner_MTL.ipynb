{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model fit\n",
      "Will fit for tasks: quaero_full, ftb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5396 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/ytaille/pyner/pyner/modules.py:402: UserWarning: Sentences > {self.max_tokens} tokens will be split with option large_sentence=\"{self.large_sentences}\". Consider using a more restrictive regex for sentence splitting if you want to avoid it.\n",
      "  warnings.warn('Sentences > {self.max_tokens} tokens will be split with option large_sentence=\"{self.large_sentences}\". Consider using a more restrictive regex for sentence splitting if you want to avoid it.')\n",
      "/home/ytaille/pyner/pyner/modules.py:402: UserWarning: Sentences > {self.max_tokens} tokens will be split with option large_sentence=\"{self.large_sentences}\". Consider using a more restrictive regex for sentence splitting if you want to avoid it.\n",
      "  warnings.warn('Sentences > {self.max_tokens} tokens will be split with option large_sentence=\"{self.large_sentences}\". Consider using a more restrictive regex for sentence splitting if you want to avoid it.')\n",
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/torch/tensor.py:904: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:930.)\n",
      "  return super(Tensor, self).rename(names)\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | preprocessor  | Preprocessor | 0     \n",
      "1 | word_encoders | ModuleList   | 177 M \n",
      "2 | decoders      | ModuleDict   | 4.2 M \n",
      "-----------------------------------------------\n",
      "182 M     Trainable params\n",
      "0         Non-trainable params\n",
      "182 M     Total params\n",
      "728.320   Total estimated model params size (MB)\n",
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> epoch </span>┃<span style=\"font-weight: bold\"> step  </span>┃<span style=\"font-weight: bold\"> train_loss </span>┃<span style=\"font-weight: bold\"> val_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_train_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_train_f1 </span>┃<span style=\"font-weight: bold\"> quaero_full_train_p </span>┃<span style=\"font-weight: bold\"> quaero_full_train_r </span>┃<span style=\"font-weight: bold\"> quaero_full_val_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_val_f1 </span>┃<span style=\"font-weight: bold\"> quaero_full_val_p </span>┃<span style=\"font-weight: bold\"> quaero_full_val_r </span>┃<span style=\"font-weight: bold\"> ftb_train_loss </span>┃<span style=\"font-weight: bold\"> ftb_train_f1 </span>┃<span style=\"font-weight: bold\"> ftb_train_p </span>┃<span style=\"font-weight: bold\"> ftb_train_r </span>┃<span style=\"font-weight: bold\"> ftb_val_loss </span>┃<span style=\"font-weight: bold\"> ftb_val_f1 </span>┃<span style=\"font-weight: bold\"> ftb_val_p </span>┃<span style=\"font-weight: bold\"> ftb_val_r </span>┃<span style=\"font-weight: bold\"> main_lr  </span>┃<span style=\"font-weight: bold\"> top_lr   </span>┃<span style=\"font-weight: bold\"> bert_lr  </span>┃\n",
       "┡━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ 0     │ 1149  │ 0.0044     │ 0.0229   │ 0.0040                 │ 0.5696               │ 0.7327              │ 0.4658              │ 0.0176               │ 0.5159             │ 0.4685            │ 0.5741            │ 0.0004         │ 0.6291       │ 0.7435      │ 0.5451      │ 0.0003       │ 0.7432     │ 0.8636    │ 0.6522    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 1     │ 2299  │ <span style=\"color: #008000\">0.0017</span>     │ <span style=\"color: #800000\">0.0244</span>   │ <span style=\"color: #008000\">0.0015</span>                 │ <span style=\"color: #008000\">0.6930</span>               │ <span style=\"color: #008000\">0.8040</span>              │ <span style=\"color: #008000\">0.6090</span>              │ <span style=\"color: #800000\">0.0192</span>               │ <span style=\"color: #800000\">0.5154</span>             │ <span style=\"color: #008000\">0.4755</span>            │ <span style=\"color: #800000\">0.5626</span>            │ <span style=\"color: #008000\">0.0002</span>         │ <span style=\"color: #008000\">0.7313</span>       │ <span style=\"color: #008000\">0.8052</span>      │ <span style=\"color: #008000\">0.6698</span>      │ <span style=\"color: #008000\">0.0002</span>       │ <span style=\"color: #008000\">0.7732</span>     │ <span style=\"color: #800000\">0.8208</span>    │ <span style=\"color: #008000\">0.7308</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 2     │ 3449  │ <span style=\"color: #008000\">0.0009</span>     │ <span style=\"color: #800000\">0.0271</span>   │ <span style=\"color: #008000\">0.0008</span>                 │ <span style=\"color: #008000\">0.7533</span>               │ <span style=\"color: #008000\">0.8413</span>              │ <span style=\"color: #008000\">0.6820</span>              │ <span style=\"color: #800000\">0.0219</span>               │ <span style=\"color: #008000\">0.5170</span>             │ <span style=\"color: #800000\">0.4713</span>            │ <span style=\"color: #800000\">0.5725</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.7827</span>       │ <span style=\"color: #008000\">0.8370</span>      │ <span style=\"color: #008000\">0.7349</span>      │ <span style=\"color: #008000\">0.0002</span>       │ <span style=\"color: #008000\">0.8003</span>     │ <span style=\"color: #800000\">0.8424</span>    │ <span style=\"color: #008000\">0.7622</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 3     │ 4599  │ <span style=\"color: #008000\">0.0006</span>     │ <span style=\"color: #800000\">0.0262</span>   │ <span style=\"color: #008000\">0.0005</span>                 │ <span style=\"color: #008000\">0.7884</span>               │ <span style=\"color: #008000\">0.8641</span>              │ <span style=\"color: #008000\">0.7249</span>              │ <span style=\"color: #800000\">0.0211</span>               │ <span style=\"color: #800000\">0.5142</span>             │ <span style=\"color: #800000\">0.4725</span>            │ <span style=\"color: #800000\">0.5639</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.8160</span>       │ <span style=\"color: #008000\">0.8591</span>      │ <span style=\"color: #008000\">0.7771</span>      │ <span style=\"color: #008000\">0.0002</span>       │ <span style=\"color: #008000\">0.8172</span>     │ <span style=\"color: #800000\">0.8518</span>    │ <span style=\"color: #008000\">0.7854</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 4     │ 5749  │ <span style=\"color: #008000\">0.0006</span>     │ <span style=\"color: #800000\">0.0293</span>   │ <span style=\"color: #008000\">0.0005</span>                 │ <span style=\"color: #008000\">0.8103</span>               │ <span style=\"color: #008000\">0.8777</span>              │ <span style=\"color: #008000\">0.7525</span>              │ <span style=\"color: #800000\">0.0241</span>               │ <span style=\"color: #800000\">0.5142</span>             │ <span style=\"color: #800000\">0.4699</span>            │ <span style=\"color: #800000\">0.5678</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.8387</span>       │ <span style=\"color: #008000\">0.8746</span>      │ <span style=\"color: #008000\">0.8057</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8257</span>     │ <span style=\"color: #800000\">0.8526</span>    │ <span style=\"color: #008000\">0.8004</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 5     │ 6899  │ <span style=\"color: #008000\">0.0005</span>     │ <span style=\"color: #800000\">0.0286</span>   │ <span style=\"color: #008000\">0.0004</span>                 │ <span style=\"color: #008000\">0.8267</span>               │ <span style=\"color: #008000\">0.8878</span>              │ <span style=\"color: #008000\">0.7734</span>              │ <span style=\"color: #800000\">0.0235</span>               │ <span style=\"color: #800000\">0.5125</span>             │ <span style=\"color: #800000\">0.4701</span>            │ <span style=\"color: #800000\">0.5632</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.8543</span>       │ <span style=\"color: #008000\">0.8854</span>      │ <span style=\"color: #008000\">0.8253</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8313</span>     │ <span style=\"color: #800000\">0.8613</span>    │ <span style=\"color: #008000\">0.8033</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 6     │ 8049  │ <span style=\"color: #008000\">0.0004</span>     │ <span style=\"color: #800000\">0.0289</span>   │ <span style=\"color: #008000\">0.0004</span>                 │ <span style=\"color: #008000\">0.8384</span>               │ <span style=\"color: #008000\">0.8949</span>              │ <span style=\"color: #008000\">0.7886</span>              │ <span style=\"color: #800000\">0.0237</span>               │ <span style=\"color: #800000\">0.5150</span>             │ <span style=\"color: #800000\">0.4729</span>            │ <span style=\"color: #800000\">0.5654</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.8674</span>       │ <span style=\"color: #008000\">0.8949</span>      │ <span style=\"color: #008000\">0.8416</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8377</span>     │ <span style=\"color: #008000\">0.8694</span>    │ <span style=\"color: #008000\">0.8084</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 7     │ 9199  │ <span style=\"color: #008000\">0.0003</span>     │ <span style=\"color: #800000\">0.0302</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8494</span>               │ <span style=\"color: #008000\">0.9018</span>              │ <span style=\"color: #008000\">0.8027</span>              │ <span style=\"color: #800000\">0.0250</span>               │ <span style=\"color: #800000\">0.5162</span>             │ <span style=\"color: #800000\">0.4735</span>            │ <span style=\"color: #800000\">0.5674</span>            │ <span style=\"color: #008000\">0.0001</span>         │ <span style=\"color: #008000\">0.8783</span>       │ <span style=\"color: #008000\">0.9027</span>      │ <span style=\"color: #008000\">0.8553</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8412</span>     │ <span style=\"color: #008000\">0.8721</span>    │ <span style=\"color: #008000\">0.8125</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 8     │ 10349 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0281</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8569</span>               │ <span style=\"color: #008000\">0.9051</span>              │ <span style=\"color: #008000\">0.8135</span>              │ <span style=\"color: #800000\">0.0230</span>               │ <span style=\"color: #800000\">0.5149</span>             │ <span style=\"color: #800000\">0.4746</span>            │ <span style=\"color: #800000\">0.5625</span>            │ <span style=\"color: #008000\">0.0000</span>         │ <span style=\"color: #008000\">0.8873</span>       │ <span style=\"color: #008000\">0.9097</span>      │ <span style=\"color: #008000\">0.8660</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8449</span>     │ <span style=\"color: #008000\">0.8726</span>    │ <span style=\"color: #008000\">0.8189</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 9     │ 11499 │ <span style=\"color: #008000\">0.0003</span>     │ <span style=\"color: #800000\">0.0305</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8648</span>               │ <span style=\"color: #008000\">0.9103</span>              │ <span style=\"color: #008000\">0.8235</span>              │ <span style=\"color: #800000\">0.0253</span>               │ <span style=\"color: #800000\">0.5165</span>             │ <span style=\"color: #008000\">0.4766</span>            │ <span style=\"color: #800000\">0.5638</span>            │ <span style=\"color: #008000\">0.0000</span>         │ <span style=\"color: #008000\">0.8948</span>       │ <span style=\"color: #008000\">0.9153</span>      │ <span style=\"color: #008000\">0.8751</span>      │ <span style=\"color: #800000\">0.0002</span>       │ <span style=\"color: #008000\">0.8477</span>     │ <span style=\"color: #008000\">0.8755</span>    │ <span style=\"color: #008000\">0.8217</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 10    │ 12649 │ <span style=\"color: #008000\">0.0002</span>     │ <span style=\"color: #800000\">0.0321</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8710</span>               │ <span style=\"color: #008000\">0.9142</span>              │ <span style=\"color: #008000\">0.8318</span>              │ <span style=\"color: #800000\">0.0270</span>               │ <span style=\"color: #800000\">0.5155</span>             │ <span style=\"color: #800000\">0.4752</span>            │ <span style=\"color: #800000\">0.5632</span>            │ <span style=\"color: #008000\">0.0000</span>         │ <span style=\"color: #008000\">0.9015</span>       │ <span style=\"color: #008000\">0.9205</span>      │ <span style=\"color: #008000\">0.8833</span>      │ <span style=\"color: #008000\">0.0002</span>       │ <span style=\"color: #008000\">0.8511</span>     │ <span style=\"color: #008000\">0.8756</span>    │ <span style=\"color: #008000\">0.8281</span>    │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "└───────┴───────┴────────────┴──────────┴────────────────────────┴──────────────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────┴───────────────────┴───────────────────┴────────────────┴──────────────┴─────────────┴─────────────┴──────────────┴────────────┴───────────┴───────────┴──────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich_logger.table_printer.RichTablePrinter at 0x7f6a578a8b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/pyner/pyner')\n",
    "\n",
    "from pyner import NER, Vocabulary, NER_MTL\n",
    "from pyner._datasets import BRATDataset\n",
    "import string\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rich_logger import RichTableLogger\n",
    "\n",
    "task_names = ['conll', 'mantra', 'n2c2', 'quaero_full', 'quaero_medline', 'quaero_emea', 'wikiann', 'cas_pos', 'ftb']\n",
    "\n",
    "# bert_name = \"bert-base-german-cased\"\n",
    "bert_name = \"bert-base-multilingual-cased\"\n",
    "# bert_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "# bert_name = 'camembert-base'\n",
    "model = NER_MTL(\n",
    "    seed=42,\n",
    "    preprocessor=dict(\n",
    "        module=\"preprocessor\",\n",
    "        bert_name=bert_name, # transformer name\n",
    "        sentence_split_regex=r\"((?:\\s*\\n)+\\s*|(?:(?<=[a-z0-9)]\\.)\\s+))(?=[A-Z-])\", # regex to use to split sentences (must not contain consuming patterns)\n",
    "        sentence_balance_chars=('()',), # try to avoid splitting between parentheses\n",
    "        sentence_entity_overlap=\"split\", # raise when an entity spans more than one sentence, or use \"split\" to split entities in 2 when this happens\n",
    "        word_regex='[\\\\w\\']+|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', # regex to use to extract words (will be aligned with bert tokens), leave to None to use wordpieces as is\n",
    "        substitutions=( # Apply these regex substitutions on sentences before tokenizing\n",
    "            (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"), # insert a space before punctuations\n",
    "            (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"), # insert a space after punctuations\n",
    "            #(\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"), # insert a space between letters and numbers\n",
    "            #(\"(?<=[0-9])(?=[A-Za-z])\", r\" \"), # insert a space between numbers and letters\n",
    "        ),\n",
    "        max_tokens=512,         # Maximum number of tokens in a sentence (will split if more than this number)\n",
    "                                # Must be equal to or lower than the max number of tokens in the Bert model\n",
    "        large_sentences=\"equal-split\", # for these large sentences, split them in equal sub sentences < max_tokens tokens \n",
    "        empty_entities=\"drop\", # when an entity cannot be mapped to any word, \"raise\" or \"drop\"\n",
    "        vocabularies=torch.nn.ModuleDict({ # vocabularies to use, call .train() before initializing to fill/complete them automatically from training data\n",
    "            \"char\": Vocabulary(string.punctuation + string.ascii_letters + string.digits, with_unk=True, with_pad=True),\n",
    "            **{f\"{task_name}_label\" : Vocabulary(with_unk=True, with_pad=False)\n",
    "                  for task_name in task_names\n",
    "              }\n",
    "        }).train(),\n",
    "    ),\n",
    "\n",
    "    # Word encore parameters\n",
    "    word_encoders=[\n",
    "        dict(\n",
    "            module=\"char_cnn\",\n",
    "            n_chars=None, # automatically inferred from data\n",
    "            in_channels=8,\n",
    "            out_channels=50,\n",
    "            kernel_sizes=(3, 4, 5),\n",
    "        ),\n",
    "        dict(\n",
    "            module=\"bert\",\n",
    "            path=bert_name,\n",
    "            n_layers=4,\n",
    "            freeze_n_layers=0, # unfreeze all\n",
    "            dropout_p=0.1,\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    # Decoder parameters\n",
    "    decoders={\n",
    "        \"quaero_full\": dict(\n",
    "        module=\"exhaustive_biaffine_ner\",\n",
    "        dim=192,\n",
    "        label_dim=64,\n",
    "        n_labels=None, # automatically inferred from data\n",
    "        dropout_p=0.,\n",
    "        use_batch_norm=False,\n",
    "        contextualizer=dict(\n",
    "            module=\"lstm\",\n",
    "            # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "            gate=dict(\n",
    "                module=\"sigmoid_gate\",\n",
    "                ln_mode=\"pre\",\n",
    "                init_value=0,\n",
    "                proj=False,\n",
    "                dim=192,\n",
    "            ),\n",
    "            input_size=768 + 150,\n",
    "            hidden_size=192,\n",
    "            num_layers=4,\n",
    "            dropout_p=0.,\n",
    "        )), \n",
    "        \"ftb\": dict(\n",
    "        module=\"exhaustive_biaffine_ner\",\n",
    "        dim=192,\n",
    "        label_dim=64,\n",
    "        n_labels=None, # automatically inferred from data\n",
    "        dropout_p=0.,\n",
    "        use_batch_norm=False,\n",
    "        contextualizer=dict(\n",
    "            module=\"lstm\",\n",
    "            # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "            gate=dict(\n",
    "                module=\"sigmoid_gate\",\n",
    "                ln_mode=\"pre\",\n",
    "                init_value=0,\n",
    "                proj=False,\n",
    "                dim=192,\n",
    "            ),\n",
    "            input_size=768 + 150,\n",
    "            hidden_size=192,\n",
    "            num_layers=4,\n",
    "            dropout_p=0.,\n",
    "        )), \n",
    "        \n",
    "#         \"quaero_medline\": dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )), \n",
    "#         \"quaero_emea\": dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )), \n",
    "        \n",
    "#         \"n2c2\":dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )),\n",
    "        \n",
    "#         \"mantra\":dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )),\n",
    "        \n",
    "#         \"wikiann\":dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )), \n",
    "        \n",
    "#         \"conll\":dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )),\n",
    "        \n",
    "#         \"cas_pos\":dict(\n",
    "#         module=\"exhaustive_biaffine_ner\",\n",
    "#         dim=192,\n",
    "#         label_dim=64,\n",
    "#         n_labels=None, # automatically inferred from data\n",
    "#         dropout_p=0.,\n",
    "#         use_batch_norm=False,\n",
    "#         contextualizer=dict(\n",
    "#             module=\"lstm\",\n",
    "#             # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "#             gate=dict(\n",
    "#                 module=\"sigmoid_gate\",\n",
    "#                 ln_mode=\"pre\",\n",
    "#                 init_value=0,\n",
    "#                 proj=False,\n",
    "#                 dim=192,\n",
    "#             ),\n",
    "#             input_size=768 + 150,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=4,\n",
    "#             dropout_p=0.,\n",
    "#         )),\n",
    "    },\n",
    "\n",
    "    # Initialize last classifying layer bias with log frequencies from labels in data\n",
    "    init_labels_bias=True,\n",
    "\n",
    "    batch_size=8,\n",
    "    \n",
    "    # Use learning rate schedules (linearly decay with warmup)\n",
    "    use_lr_schedules=False,\n",
    "    warmup_rate=0.1,\n",
    "\n",
    "    gradient_clip_val=5.,\n",
    "    \n",
    "    # Learning rates\n",
    "    main_lr=1.5e-3,\n",
    "    top_lr=1.5e-3,\n",
    "    bert_lr=4e-5,\n",
    "    \n",
    "    # Optimizer, can be class or str\n",
    "    optimizer_cls=\"transformers.AdamW\",\n",
    "    \n",
    "    share_contextualizers=\"hybrid\",\n",
    ").train()\n",
    "\n",
    "flt_format = (5, \"{:.4f}\".format)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    progress_bar_refresh_rate=False,\n",
    "    move_metrics_to_cpu=True,\n",
    "    logger=[\n",
    "        #        pl.loggers.TestTubeLogger(\"path/to/logs\", name=\"my_experiment\"),\n",
    "        RichTableLogger(key=\"epoch\", fields={\n",
    "            \"epoch\": {},\n",
    "            \"step\": {},\n",
    "            \"train_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": \"train_loss\"},\n",
    "            \"val_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": \"val_loss\"},\n",
    "            **{k:v \n",
    "            for task_name in model.decoders.keys() for k,v in {\n",
    "            f\"{task_name}_train_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_loss\"},\n",
    "            f\"{task_name}_train_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_f1\"},\n",
    "            f\"{task_name}_train_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_p\"},\n",
    "            f\"{task_name}_train_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_r\"},\n",
    "\n",
    "            f\"{task_name}_val_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_loss\"},\n",
    "            f\"{task_name}_val_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_f1\"},\n",
    "            f\"{task_name}_val_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_p\"},\n",
    "            f\"{task_name}_val_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_r\"},\n",
    "            }.items()},\n",
    "\n",
    "            \"main_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"top_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"bert_lr\": {\"format\": \"{:.2e}\"},\n",
    "        },\n",
    "       ),\n",
    "    ],\n",
    "    max_epochs=50)\n",
    "\n",
    "\n",
    "# N2C2 PATH:\n",
    "# /home/ytaille/data/resources/n2c2/brat_files/train\n",
    "\n",
    "# QUAERO PATH:\n",
    "# /home/ytaille/data/resources/quaero/corpus/train/MEDLINE\n",
    "\n",
    "# MANTRA PATH:\n",
    "# /home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\n",
    "# /home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\n",
    "\n",
    "# CONLL PATH:\n",
    "# /home/ytaille/data/resources/conll/brat/eng/train\n",
    "\n",
    "dropped_entity_label = [] #['ACTI', 'CONC', 'GEOG']\n",
    "\n",
    "# conll_dataset = BRATDataset(\n",
    "#     train=[\n",
    "#         \"/home/ytaille/data/resources/conll/brat/eng/train\",\n",
    "#     ],\n",
    "#     test=\"/home/ytaille/data/resources/conll/brat/eng/test\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "#     val=\"/home/ytaille/data/resources/conll/brat/eng/dev\", # first 20% doc will be for validation\n",
    "#     seed=42,  # don't shuffle before splitting1\n",
    "#     dropped_entity_label=dropped_entity_label,\n",
    "# )\n",
    "\n",
    "# n2c2_dataset = BRATDataset(\n",
    "#     train=[\n",
    "#         \"/home/ytaille/data/resources/n2c2/brat_files/train\",\n",
    "#     ],\n",
    "#     test=\"/home/ytaille/data/resources/n2c2/brat_files/test\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "#     val=\"/home/ytaille/data/resources/n2c2/brat_files/test\", # first 20% doc will be for validation\n",
    "#     seed=42,  # don't shuffle before splitting\n",
    "#     dropped_entity_label=dropped_entity_label,\n",
    "# )\n",
    "\n",
    "ftb_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/french_treebank/ftb6/brat/train\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/french_treebank/ftb6/brat/test\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=[\n",
    "        \"/home/ytaille/data/resources/french_treebank/ftb6/brat/dev\",\n",
    "        \n",
    "    ],# first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "quaero_full_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/quaero/corpus/train/MEDLINE\",\n",
    "        \"/home/ytaille/data/resources/quaero/corpus/train/EMEA\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/quaero/corpus/test/MEDLINE\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=[\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\",\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\",\n",
    "        \n",
    "    ],# first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "quaero_medline_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/quaero/corpus/train/MEDLINE\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/quaero/corpus/test/MEDLINE\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=[\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\",\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\",\n",
    "        \n",
    "    ],# first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "quaero_emea_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/quaero/corpus/train/EMEA\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/quaero/corpus/test/EMEA\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=[\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\",\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\",\n",
    "        \n",
    "    ],# first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "mantra_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=\"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\", # first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "# wikiann_dataset = BRATDataset(\n",
    "#     train=[\n",
    "#         \"/home/ytaille/data/resources/wikiann/fr/train\",\n",
    "#     ],\n",
    "#     test=\"/home/ytaille/data/resources/wikiann/fr/test\",# None for training only, test directory otherwise\n",
    "#     val=\"/home/ytaille/data/resources/wikiann/fr/val\", # first 20% doc will be for validation\n",
    "#     seed=42,  # don't shuffle before splitting\n",
    "#     dropped_entity_label=dropped_entity_label,\n",
    "# )\n",
    "\n",
    "cas_pos_dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/corpus_dalloux/CAS_POS_brat\",\n",
    "    ],\n",
    "    test=\"/home/ytaille/data/resources/corpus_dalloux/CAS_POS_brat\", \n",
    "    val=\"/home/ytaille/data/resources/corpus_dalloux/CAS_POS_brat\", \n",
    "    seed=42,  # don't shuffle before splitting\n",
    "    dropped_entity_label=dropped_entity_label,\n",
    ")\n",
    "\n",
    "# PRENDRE LSTM EN COMMUN AU LIEU DE DIFFÉRENCER LES DECODERS ENTIERS (seule la dernière couche est spécifique aux tâches)\n",
    "\n",
    "# PROBLEM IN VAL LOADING -> use CombinedLoader -> OK\n",
    "\n",
    "MTL_data = {\n",
    "    \"quaero_full\": quaero_full_dataset,\n",
    "    \"ftb\": ftb_dataset,\n",
    "#     \"quaero_medline\": quaero_medline_dataset,\n",
    "#     \"quaero_emea\": quaero_emea_dataset,\n",
    "#     \"mantra\": mantra_dataset,\n",
    "#     \"cas_pos\": cas_pos_dataset,\n",
    "#     \"wikiann\": wikiann_dataset,\n",
    "#     \"conll\": conll_dataset,\n",
    "#     \"n2c2\": n2c2_dataset,\n",
    "}\n",
    "\n",
    "# assert model.decoders.keys() == MTL_data.keys(), f\"Datasets and decoders tasks don't match.\\n Datasets:{MTL_data.keys()}\\nDecoders:{model.decoders.keys()}\"\n",
    "\n",
    "try:\n",
    "    print(\"Starting model fit\")\n",
    "    print(f\"Will fit for tasks: {', '.join(model.decoders.keys())}\")\n",
    "    trainer.fit(model, MTL_data)\n",
    "\n",
    "    # Save logs and config\n",
    "\n",
    "    from datetime import datetime\n",
    "    import pathlib\n",
    "    import os\n",
    "    \n",
    "    log_dir = \"/home/ytaille/pyner/logs/\"\n",
    "    exp_dir = os.path.join(log_dir, \"_\".join(MTL_data.keys()))\n",
    "    exp_sub_dir = os.path.join(exp_dir, datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\"))\n",
    "\n",
    "    pathlib.Path(exp_sub_dir).mkdir(parents=True, exist_ok=True)\n",
    "    table_html_file = os.path.join(exp_sub_dir, \"table.html\")\n",
    "    config_file = os.path.join(exp_sub_dir, \"config.json\")\n",
    "\n",
    "    console = trainer.logger[0].printer.console\n",
    "    table = trainer.logger[0].printer.table\n",
    "    with console.capture() as capture: \n",
    "        console.print(table)\n",
    "    console.save_html(table_html_file)\n",
    "    \n",
    "    from torch_utils import get_config\n",
    "    import json\n",
    "    with open(config_file, 'w') as fp:\n",
    "        json.dump(get_config(model), fp)\n",
    "    \n",
    "except:\n",
    "    print(\"SOMETHING FAILED DURING FIT\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import pathlib\n",
    "    import os\n",
    "    log_dir = \"/home/ytaille/pyner/logs/\"\n",
    "    exp_dir = os.path.join(log_dir, \"_\".join(MTL_data.keys()))\n",
    "    exp_sub_dir = os.path.join(exp_dir, datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\"))\n",
    "\n",
    "    pathlib.Path(exp_sub_dir).mkdir(parents=True, exist_ok=True)\n",
    "    table_html_file = os.path.join(exp_sub_dir, \"table.html\")\n",
    "    config_file = os.path.join(exp_sub_dir, \"config.json\")\n",
    "\n",
    "    console = trainer.logger[0].printer.console\n",
    "    table = trainer.logger[0].printer.table\n",
    "    with console.capture() as capture:\n",
    "        console.print(table)\n",
    "    console.save_html(table_html_file)\n",
    "    \n",
    "    from torch_utils import get_config\n",
    "    import json\n",
    "    with open(config_file, 'w') as fp:\n",
    "        json.dump(get_config(model), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name          | Type         | Params\n",
      "-----------------------------------------------\n",
      "0 | preprocessor  | Preprocessor | 0     \n",
      "1 | word_encoders | ModuleList   | 177 M \n",
      "2 | decoders      | ModuleDict   | 2.7 M \n",
      "-----------------------------------------------\n",
      "180 M     Trainable params\n",
      "0         Non-trainable params\n",
      "180 M     Total params\n",
      "722.336   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> epoch </span>┃<span style=\"font-weight: bold\"> step </span>┃<span style=\"font-weight: bold\"> train_loss </span>┃<span style=\"font-weight: bold\"> val_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_train_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_train_f1 </span>┃<span style=\"font-weight: bold\"> quaero_full_train_p </span>┃<span style=\"font-weight: bold\"> quaero_full_train_r </span>┃<span style=\"font-weight: bold\"> quaero_full_val_loss </span>┃<span style=\"font-weight: bold\"> quaero_full_val_f1 </span>┃<span style=\"font-weight: bold\"> quaero_full_val_p </span>┃<span style=\"font-weight: bold\"> quaero_full_val_r </span>┃<span style=\"font-weight: bold\"> main_lr  </span>┃<span style=\"font-weight: bold\"> top_lr   </span>┃<span style=\"font-weight: bold\"> bert_lr  </span>┃\n",
       "┡━━━━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ 0     │ 159  │ 0.0005     │ 0.0244   │ 0.0005                 │ 0.8834               │ 0.9217              │ 0.8482              │ 0.0244               │ 0.5086             │ 0.4727            │ 0.5504            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 1     │ 319  │ <span style=\"color: #008000\">0.0004</span>     │ <span style=\"color: #800000\">0.0263</span>   │ <span style=\"color: #008000\">0.0004</span>                 │ <span style=\"color: #008000\">0.8842</span>               │ <span style=\"color: #008000\">0.9222</span>              │ <span style=\"color: #008000\">0.8492</span>              │ <span style=\"color: #800000\">0.0263</span>               │ <span style=\"color: #008000\">0.5089</span>             │ <span style=\"color: #008000\">0.4728</span>            │ <span style=\"color: #008000\">0.5510</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 2     │ 479  │ <span style=\"color: #008000\">0.0003</span>     │ <span style=\"color: #800000\">0.0272</span>   │ <span style=\"color: #008000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8849</span>               │ <span style=\"color: #008000\">0.9227</span>              │ <span style=\"color: #008000\">0.8502</span>              │ <span style=\"color: #800000\">0.0272</span>               │ <span style=\"color: #008000\">0.5091</span>             │ <span style=\"color: #008000\">0.4729</span>            │ <span style=\"color: #008000\">0.5514</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 3     │ 639  │ <span style=\"color: #008000\">0.0003</span>     │ <span style=\"color: #800000\">0.0269</span>   │ <span style=\"color: #008000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8856</span>               │ <span style=\"color: #008000\">0.9231</span>              │ <span style=\"color: #008000\">0.8510</span>              │ <span style=\"color: #800000\">0.0269</span>               │ <span style=\"color: #008000\">0.5092</span>             │ <span style=\"color: #008000\">0.4729</span>            │ <span style=\"color: #008000\">0.5516</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 4     │ 799  │ <span style=\"color: #008000\">0.0003</span>     │ <span style=\"color: #800000\">0.0262</span>   │ <span style=\"color: #008000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8862</span>               │ <span style=\"color: #008000\">0.9235</span>              │ <span style=\"color: #008000\">0.8518</span>              │ <span style=\"color: #800000\">0.0262</span>               │ <span style=\"color: #008000\">0.5096</span>             │ <span style=\"color: #008000\">0.4732</span>            │ <span style=\"color: #008000\">0.5520</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 5     │ 959  │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0247</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8867</span>               │ <span style=\"color: #008000\">0.9238</span>              │ <span style=\"color: #008000\">0.8525</span>              │ <span style=\"color: #800000\">0.0247</span>               │ <span style=\"color: #008000\">0.5097</span>             │ <span style=\"color: #008000\">0.4733</span>            │ <span style=\"color: #008000\">0.5521</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 6     │ 1119 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0266</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8872</span>               │ <span style=\"color: #008000\">0.9241</span>              │ <span style=\"color: #008000\">0.8531</span>              │ <span style=\"color: #800000\">0.0266</span>               │ <span style=\"color: #800000\">0.5096</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5520</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 7     │ 1279 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0285</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8876</span>               │ <span style=\"color: #008000\">0.9243</span>              │ <span style=\"color: #008000\">0.8538</span>              │ <span style=\"color: #800000\">0.0285</span>               │ <span style=\"color: #008000\">0.5097</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #008000\">0.5525</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 8     │ 1439 │ <span style=\"color: #008000\">0.0002</span>     │ <span style=\"color: #800000\">0.0281</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8882</span>               │ <span style=\"color: #008000\">0.9246</span>              │ <span style=\"color: #008000\">0.8545</span>              │ <span style=\"color: #800000\">0.0281</span>               │ <span style=\"color: #008000\">0.5097</span>             │ <span style=\"color: #800000\">0.4730</span>            │ <span style=\"color: #008000\">0.5527</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 9     │ 1599 │ <span style=\"color: #800000\">0.0004</span>     │ <span style=\"color: #800000\">0.0258</span>   │ <span style=\"color: #800000\">0.0004</span>                 │ <span style=\"color: #008000\">0.8885</span>               │ <span style=\"color: #008000\">0.9248</span>              │ <span style=\"color: #008000\">0.8550</span>              │ <span style=\"color: #800000\">0.0258</span>               │ <span style=\"color: #800000\">0.5097</span>             │ <span style=\"color: #800000\">0.4730</span>            │ <span style=\"color: #800000\">0.5525</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 10    │ 1759 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0265</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8888</span>               │ <span style=\"color: #800000\">0.9247</span>              │ <span style=\"color: #008000\">0.8556</span>              │ <span style=\"color: #800000\">0.0265</span>               │ <span style=\"color: #008000\">0.5098</span>             │ <span style=\"color: #800000\">0.4729</span>            │ <span style=\"color: #008000\">0.5529</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 11    │ 1919 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0270</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8891</span>               │ <span style=\"color: #008000\">0.9250</span>              │ <span style=\"color: #008000\">0.8559</span>              │ <span style=\"color: #800000\">0.0270</span>               │ <span style=\"color: #008000\">0.5098</span>             │ <span style=\"color: #800000\">0.4728</span>            │ <span style=\"color: #008000\">0.5531</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 12    │ 2079 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0253</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.8895</span>               │ <span style=\"color: #008000\">0.9253</span>              │ <span style=\"color: #008000\">0.8564</span>              │ <span style=\"color: #800000\">0.0253</span>               │ <span style=\"color: #008000\">0.5100</span>             │ <span style=\"color: #800000\">0.4730</span>            │ <span style=\"color: #008000\">0.5532</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 13    │ 2239 │ <span style=\"color: #008000\">0.0002</span>     │ <span style=\"color: #800000\">0.0264</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8900</span>               │ <span style=\"color: #008000\">0.9255</span>              │ <span style=\"color: #008000\">0.8570</span>              │ <span style=\"color: #800000\">0.0264</span>               │ <span style=\"color: #008000\">0.5102</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #008000\">0.5535</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 14    │ 2399 │ <span style=\"color: #008000\">0.0002</span>     │ <span style=\"color: #800000\">0.0279</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8905</span>               │ <span style=\"color: #008000\">0.9259</span>              │ <span style=\"color: #008000\">0.8577</span>              │ <span style=\"color: #800000\">0.0279</span>               │ <span style=\"color: #008000\">0.5104</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #008000\">0.5538</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 15    │ 2559 │ <span style=\"color: #008000\">0.0002</span>     │ <span style=\"color: #800000\">0.0269</span>   │ <span style=\"color: #008000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8910</span>               │ <span style=\"color: #008000\">0.9263</span>              │ <span style=\"color: #008000\">0.8583</span>              │ <span style=\"color: #800000\">0.0269</span>               │ <span style=\"color: #008000\">0.5107</span>             │ <span style=\"color: #008000\">0.4735</span>            │ <span style=\"color: #008000\">0.5541</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 16    │ 2719 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0262</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8915</span>               │ <span style=\"color: #008000\">0.9267</span>              │ <span style=\"color: #008000\">0.8589</span>              │ <span style=\"color: #800000\">0.0262</span>               │ <span style=\"color: #008000\">0.5108</span>             │ <span style=\"color: #008000\">0.4737</span>            │ <span style=\"color: #008000\">0.5542</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 17    │ 2879 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0263</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8920</span>               │ <span style=\"color: #008000\">0.9269</span>              │ <span style=\"color: #008000\">0.8596</span>              │ <span style=\"color: #800000\">0.0263</span>               │ <span style=\"color: #008000\">0.5108</span>             │ <span style=\"color: #008000\">0.4738</span>            │ <span style=\"color: #800000\">0.5541</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 18    │ 3039 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0271</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8923</span>               │ <span style=\"color: #008000\">0.9271</span>              │ <span style=\"color: #008000\">0.8600</span>              │ <span style=\"color: #800000\">0.0271</span>               │ <span style=\"color: #800000\">0.5108</span>             │ <span style=\"color: #800000\">0.4737</span>            │ <span style=\"color: #008000\">0.5543</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 19    │ 3199 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0295</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8928</span>               │ <span style=\"color: #008000\">0.9273</span>              │ <span style=\"color: #008000\">0.8607</span>              │ <span style=\"color: #800000\">0.0295</span>               │ <span style=\"color: #800000\">0.5108</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #008000\">0.5547</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 20    │ 3359 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0273</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8932</span>               │ <span style=\"color: #008000\">0.9276</span>              │ <span style=\"color: #008000\">0.8613</span>              │ <span style=\"color: #800000\">0.0273</span>               │ <span style=\"color: #008000\">0.5109</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #008000\">0.5550</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 21    │ 3519 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0270</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8936</span>               │ <span style=\"color: #008000\">0.9279</span>              │ <span style=\"color: #008000\">0.8618</span>              │ <span style=\"color: #800000\">0.0270</span>               │ <span style=\"color: #008000\">0.5110</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #008000\">0.5552</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 22    │ 3679 │ <span style=\"color: #008000\">0.0001</span>     │ <span style=\"color: #800000\">0.0258</span>   │ <span style=\"color: #008000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8942</span>               │ <span style=\"color: #008000\">0.9283</span>              │ <span style=\"color: #008000\">0.8625</span>              │ <span style=\"color: #800000\">0.0258</span>               │ <span style=\"color: #008000\">0.5110</span>             │ <span style=\"color: #800000\">0.4734</span>            │ <span style=\"color: #800000\">0.5550</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 23    │ 3839 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0274</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8947</span>               │ <span style=\"color: #008000\">0.9287</span>              │ <span style=\"color: #008000\">0.8632</span>              │ <span style=\"color: #800000\">0.0274</span>               │ <span style=\"color: #008000\">0.5110</span>             │ <span style=\"color: #800000\">0.4734</span>            │ <span style=\"color: #800000\">0.5551</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 24    │ 3999 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0281</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8952</span>               │ <span style=\"color: #008000\">0.9289</span>              │ <span style=\"color: #008000\">0.8638</span>              │ <span style=\"color: #800000\">0.0281</span>               │ <span style=\"color: #008000\">0.5110</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #008000\">0.5554</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 25    │ 4159 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0268</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8956</span>               │ <span style=\"color: #008000\">0.9292</span>              │ <span style=\"color: #008000\">0.8644</span>              │ <span style=\"color: #800000\">0.0268</span>               │ <span style=\"color: #800000\">0.5110</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #008000\">0.5555</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 26    │ 4319 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0258</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8961</span>               │ <span style=\"color: #008000\">0.9295</span>              │ <span style=\"color: #008000\">0.8650</span>              │ <span style=\"color: #800000\">0.0258</span>               │ <span style=\"color: #008000\">0.5110</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #800000\">0.5554</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 27    │ 4479 │ <span style=\"color: #008000\">0.0001</span>     │ <span style=\"color: #800000\">0.0267</span>   │ <span style=\"color: #008000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8966</span>               │ <span style=\"color: #008000\">0.9298</span>              │ <span style=\"color: #008000\">0.8657</span>              │ <span style=\"color: #800000\">0.0267</span>               │ <span style=\"color: #008000\">0.5112</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #008000\">0.5556</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 28    │ 4639 │ <span style=\"color: #008000\">0.0001</span>     │ <span style=\"color: #800000\">0.0274</span>   │ <span style=\"color: #008000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8972</span>               │ <span style=\"color: #008000\">0.9302</span>              │ <span style=\"color: #008000\">0.8664</span>              │ <span style=\"color: #800000\">0.0274</span>               │ <span style=\"color: #008000\">0.5112</span>             │ <span style=\"color: #800000\">0.4734</span>            │ <span style=\"color: #008000\">0.5556</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 29    │ 4799 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0305</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8977</span>               │ <span style=\"color: #008000\">0.9305</span>              │ <span style=\"color: #008000\">0.8672</span>              │ <span style=\"color: #800000\">0.0305</span>               │ <span style=\"color: #008000\">0.5112</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #008000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 30    │ 4959 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0272</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8981</span>               │ <span style=\"color: #008000\">0.9308</span>              │ <span style=\"color: #008000\">0.8677</span>              │ <span style=\"color: #800000\">0.0272</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 31    │ 5119 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0271</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8986</span>               │ <span style=\"color: #008000\">0.9311</span>              │ <span style=\"color: #008000\">0.8683</span>              │ <span style=\"color: #800000\">0.0271</span>               │ <span style=\"color: #008000\">0.5113</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 32    │ 5279 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0276</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8991</span>               │ <span style=\"color: #008000\">0.9314</span>              │ <span style=\"color: #008000\">0.8689</span>              │ <span style=\"color: #800000\">0.0276</span>               │ <span style=\"color: #008000\">0.5114</span>             │ <span style=\"color: #800000\">0.4734</span>            │ <span style=\"color: #008000\">0.5560</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 33    │ 5439 │ <span style=\"color: #008000\">0.0001</span>     │ <span style=\"color: #800000\">0.0276</span>   │ <span style=\"color: #008000\">0.0001</span>                 │ <span style=\"color: #008000\">0.8995</span>               │ <span style=\"color: #008000\">0.9317</span>              │ <span style=\"color: #008000\">0.8696</span>              │ <span style=\"color: #800000\">0.0276</span>               │ <span style=\"color: #008000\">0.5115</span>             │ <span style=\"color: #800000\">0.4735</span>            │ <span style=\"color: #008000\">0.5561</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 34    │ 5599 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0303</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.8999</span>               │ <span style=\"color: #008000\">0.9318</span>              │ <span style=\"color: #008000\">0.8700</span>              │ <span style=\"color: #800000\">0.0303</span>               │ <span style=\"color: #800000\">0.5113</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #008000\">0.5561</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 35    │ 5759 │ <span style=\"color: #800000\">0.0003</span>     │ <span style=\"color: #800000\">0.0280</span>   │ <span style=\"color: #800000\">0.0003</span>                 │ <span style=\"color: #008000\">0.9001</span>               │ <span style=\"color: #008000\">0.9319</span>              │ <span style=\"color: #008000\">0.8704</span>              │ <span style=\"color: #800000\">0.0280</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #800000\">0.5558</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 36    │ 5919 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0288</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.9004</span>               │ <span style=\"color: #008000\">0.9321</span>              │ <span style=\"color: #008000\">0.8707</span>              │ <span style=\"color: #800000\">0.0288</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 37    │ 6079 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0295</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.9007</span>               │ <span style=\"color: #008000\">0.9322</span>              │ <span style=\"color: #008000\">0.8712</span>              │ <span style=\"color: #800000\">0.0295</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 38    │ 6239 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0281</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.9010</span>               │ <span style=\"color: #008000\">0.9324</span>              │ <span style=\"color: #008000\">0.8716</span>              │ <span style=\"color: #800000\">0.0281</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4731</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 39    │ 6399 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0278</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9013</span>               │ <span style=\"color: #008000\">0.9327</span>              │ <span style=\"color: #008000\">0.8720</span>              │ <span style=\"color: #800000\">0.0278</span>               │ <span style=\"color: #800000\">0.5112</span>             │ <span style=\"color: #800000\">0.4732</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 40    │ 6559 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0281</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9017</span>               │ <span style=\"color: #008000\">0.9328</span>              │ <span style=\"color: #008000\">0.8726</span>              │ <span style=\"color: #800000\">0.0281</span>               │ <span style=\"color: #800000\">0.5113</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5560</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 41    │ 6719 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0282</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9021</span>               │ <span style=\"color: #008000\">0.9331</span>              │ <span style=\"color: #008000\">0.8730</span>              │ <span style=\"color: #800000\">0.0282</span>               │ <span style=\"color: #800000\">0.5113</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5560</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 42    │ 6879 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0272</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9024</span>               │ <span style=\"color: #008000\">0.9333</span>              │ <span style=\"color: #008000\">0.8736</span>              │ <span style=\"color: #800000\">0.0272</span>               │ <span style=\"color: #800000\">0.5114</span>             │ <span style=\"color: #800000\">0.4734</span>            │ <span style=\"color: #800000\">0.5559</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 43    │ 7039 │ <span style=\"color: #800000\">0.0002</span>     │ <span style=\"color: #800000\">0.0276</span>   │ <span style=\"color: #800000\">0.0002</span>                 │ <span style=\"color: #008000\">0.9027</span>               │ <span style=\"color: #008000\">0.9334</span>              │ <span style=\"color: #008000\">0.8739</span>              │ <span style=\"color: #800000\">0.0276</span>               │ <span style=\"color: #800000\">0.5113</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5560</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 44    │ 7199 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0288</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9031</span>               │ <span style=\"color: #008000\">0.9336</span>              │ <span style=\"color: #008000\">0.8744</span>              │ <span style=\"color: #800000\">0.0288</span>               │ <span style=\"color: #800000\">0.5114</span>             │ <span style=\"color: #800000\">0.4733</span>            │ <span style=\"color: #800000\">0.5561</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 45    │ 7359 │ <span style=\"color: #008000\">0.0001</span>     │ <span style=\"color: #800000\">0.0272</span>   │ <span style=\"color: #008000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9035</span>               │ <span style=\"color: #008000\">0.9339</span>              │ <span style=\"color: #008000\">0.8750</span>              │ <span style=\"color: #800000\">0.0272</span>               │ <span style=\"color: #800000\">0.5114</span>             │ <span style=\"color: #800000\">0.4735</span>            │ <span style=\"color: #800000\">0.5560</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 46    │ 7519 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0289</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9039</span>               │ <span style=\"color: #008000\">0.9341</span>              │ <span style=\"color: #008000\">0.8755</span>              │ <span style=\"color: #800000\">0.0289</span>               │ <span style=\"color: #008000\">0.5115</span>             │ <span style=\"color: #800000\">0.4735</span>            │ <span style=\"color: #008000\">0.5563</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 47    │ 7679 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0282</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9043</span>               │ <span style=\"color: #008000\">0.9344</span>              │ <span style=\"color: #008000\">0.8760</span>              │ <span style=\"color: #800000\">0.0282</span>               │ <span style=\"color: #800000\">0.5115</span>             │ <span style=\"color: #800000\">0.4735</span>            │ <span style=\"color: #800000\">0.5562</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 48    │ 7839 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0268</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9047</span>               │ <span style=\"color: #008000\">0.9347</span>              │ <span style=\"color: #008000\">0.8765</span>              │ <span style=\"color: #800000\">0.0268</span>               │ <span style=\"color: #008000\">0.5116</span>             │ <span style=\"color: #800000\">0.4736</span>            │ <span style=\"color: #800000\">0.5561</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "│ 49    │ 7999 │ <span style=\"color: #800000\">0.0001</span>     │ <span style=\"color: #800000\">0.0291</span>   │ <span style=\"color: #800000\">0.0001</span>                 │ <span style=\"color: #008000\">0.9050</span>               │ <span style=\"color: #008000\">0.9349</span>              │ <span style=\"color: #008000\">0.8770</span>              │ <span style=\"color: #800000\">0.0291</span>               │ <span style=\"color: #008000\">0.5117</span>             │ <span style=\"color: #800000\">0.4738</span>            │ <span style=\"color: #800000\">0.5563</span>            │ 1.50e-03 │ 4.00e-05 │ 1.50e-03 │\n",
       "└───────┴──────┴────────────┴──────────┴────────────────────────┴──────────────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────┴───────────────────┴───────────────────┴──────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich_logger.table_printer.RichTablePrinter at 0x7fb34203f850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    progress_bar_refresh_rate=False,\n",
    "    move_metrics_to_cpu=True,\n",
    "    logger=[\n",
    "        #        pl.loggers.TestTubeLogger(\"path/to/logs\", name=\"my_experiment\"),\n",
    "        RichTableLogger(key=\"epoch\", fields={\n",
    "            \"epoch\": {},\n",
    "            \"step\": {},\n",
    "            \"train_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": \"train_loss\"},\n",
    "            \"val_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": \"val_loss\"},\n",
    "            **{k:v \n",
    "            for task_name in model.decoders.keys() for k,v in {\n",
    "            f\"{task_name}_train_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_loss\"},\n",
    "            f\"{task_name}_train_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_f1\"},\n",
    "            f\"{task_name}_train_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_p\"},\n",
    "            f\"{task_name}_train_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_train_r\"},\n",
    "\n",
    "            f\"{task_name}_val_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_loss\"},\n",
    "            f\"{task_name}_val_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_f1\"},\n",
    "            f\"{task_name}_val_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_p\"},\n",
    "            f\"{task_name}_val_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": f\"{task_name}_val_r\"},\n",
    "            }.items()},\n",
    "\n",
    "            \"main_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"top_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"bert_lr\": {\"format\": \"{:.2e}\"},\n",
    "        },\n",
    "       ),\n",
    "    ],\n",
    "    max_epochs=50)\n",
    "\n",
    "trainer.fit(model, MTL_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTL -> mask labels au lieu de faire des têtes différentes\n",
    "# Aligner labels multilingues\n",
    "# Méga entraînement avec tous les datasets et toutes les tâches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f2ca3c10cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pathlib\n",
    "import os\n",
    "log_dir = \"/home/ytaille/pyner/logs/\"\n",
    "exp_dir = os.path.join(log_dir, \"_\".join(MTL_data.keys()))\n",
    "exp_sub_dir = os.path.join(exp_dir, datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\"))\n",
    "\n",
    "pathlib.Path(exp_sub_dir).mkdir(parents=True, exist_ok=True)\n",
    "table_html_file = os.path.join(exp_sub_dir, \"table.html\")\n",
    "config_file = os.path.join(exp_sub_dir, \"config.json\")\n",
    "\n",
    "console = trainer.logger[0].printer.console\n",
    "table = trainer.logger[0].printer.table\n",
    "with console.capture() as capture:\n",
    "    console.print(table)\n",
    "console.save_html(table_html_file)\n",
    "\n",
    "from torch_utils import get_config\n",
    "import json\n",
    "with open(config_file, 'w') as fp:\n",
    "    json.dump(get_config(model), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEME QUAND VAL=NONE dans bratdataset -> preprocess has no attribute \"chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
