{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify  MANTRA BRAT files\n",
    "\n",
    "# from glob import glob\n",
    "# from pathlib import Path\n",
    "# from shutil import copyfile\n",
    "# path = \"/home/ytaille/data/resources/mantra/Mantra-GSC\"\n",
    "\n",
    "# all_files = glob(path + '/*/*/*.ann')\n",
    "\n",
    "# for file_path in all_files:\n",
    "#     with open(file_path, 'r') as fp:\n",
    "#         annotations = fp.readlines()\n",
    "#     new_annotations = \"\"\n",
    "#     add_comment = False\n",
    "#     for i, ann in enumerate(annotations):\n",
    "#         if ann[0] == \"T\":\n",
    "#             ann_id, ann_label_b_e, ann_text = ann.split('\\t')\n",
    "#             new_ann_label = annotations[i+1].split(',')[-1].strip('\" \\n\"')\n",
    "#             if \"Manufactured Object\" not in new_ann_label:\n",
    "#                 new_ann_label_b_e = \" \".join([new_ann_label] + ann_label_b_e.split(' ')[1:])\n",
    "\n",
    "#                 new_annotations += (f\"{ann_id}\\t{new_ann_label_b_e}\\t{ann_text}\")\n",
    "#                 add_comment = True\n",
    "#         else:\n",
    "#             if add_comment:\n",
    "#                 new_annotations += ann\n",
    "#                 add_comment = False\n",
    "            \n",
    "#     new_ann_path = Path(file_path.replace('Mantra-GSC','Mantra-GSC_new_ann'))\n",
    "#     new_ann_path.parent.absolute().mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     old_txt_path = file_path.replace('.ann', '.txt')\n",
    "#     new_txt_path = old_txt_path.replace('Mantra-GSC','Mantra-GSC_new_ann')\n",
    "    \n",
    "    \n",
    "#     copyfile(old_txt_path, new_txt_path)\n",
    "#     with open(new_ann_path, 'w') as f:\n",
    "#         f.write(new_annotations)\n",
    "# export_to_brat(model.predict(load_from_brat(\"/heome/ytaille/data/resources/quaero/corpus/test\")), filename_prefix=\"/home/ytaille/data/resources/quaero/predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT RICH TABLE LOGGER YT_NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/torch/tensor.py:904: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:930.)\n",
      "  return super(Tensor, self).rename(names)\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name          | Type                         | Params\n",
      "---------------------------------------------------------------\n",
      "0 | train_metric  | PrecisionRecallF1Metric      | 0     \n",
      "1 | val_metric    | PrecisionRecallF1Metric      | 0     \n",
      "2 | test_metric   | PrecisionRecallF1Metric      | 0     \n",
      "3 | preprocessor  | Preprocessor                 | 0     \n",
      "4 | word_encoders | ModuleList                   | 177 M \n",
      "5 | decoder       | ExhaustiveBiaffineNERDecoder | 1.4 M \n",
      "---------------------------------------------------------------\n",
      "179 M     Trainable params\n",
      "0         Non-trainable params\n",
      "179 M     Total params\n",
      "716.979   Total estimated model params size (MB)\n",
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> epoch </span>┃<span style=\"font-weight: bold\"> step </span>┃<span style=\"font-weight: bold\"> train_loss </span>┃<span style=\"font-weight: bold\"> train_f1 </span>┃<span style=\"font-weight: bold\"> train_p </span>┃<span style=\"font-weight: bold\"> train_r </span>┃<span style=\"font-weight: bold\"> val_loss </span>┃<span style=\"font-weight: bold\"> val_f1 </span>┃<span style=\"font-weight: bold\"> val_p  </span>┃<span style=\"font-weight: bold\"> val_r  </span>┃<span style=\"font-weight: bold\"> main_lr  </span>┃<span style=\"font-weight: bold\"> top_lr   </span>┃<span style=\"font-weight: bold\"> bert_lr  </span>┃\n",
       "┡━━━━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ 0     │ 6    │ nan        │ 0.0000   │ 0.0000  │ 0.0000  │ nan      │ 0.0000 │ 0.0000 │ 0.0000 │ 1.47e-03 │ 1.47e-03 │ 8.00e-06 │\n",
       "│ 1     │ 13   │ nan        │ <span style=\"color: #800000\">0.0000</span>   │ <span style=\"color: #800000\">0.0000</span>  │ <span style=\"color: #800000\">0.0000</span>  │ nan      │ <span style=\"color: #800000\">0.0000</span> │ <span style=\"color: #800000\">0.0000</span> │ <span style=\"color: #800000\">0.0000</span> │ 1.44e-03 │ 1.44e-03 │ 1.60e-05 │\n",
       "│ 2     │ 20   │ nan        │ <span style=\"color: #800000\">0.0000</span>   │ <span style=\"color: #800000\">0.0000</span>  │ <span style=\"color: #800000\">0.0000</span>  │ nan      │ <span style=\"color: #800000\">0.0000</span> │ <span style=\"color: #800000\">0.0000</span> │ <span style=\"color: #800000\">0.0000</span> │ 1.41e-03 │ 1.41e-03 │ 2.40e-05 │\n",
       "│ 3     │ 27   │ nan        │ <span style=\"color: #008000\">0.0078</span>   │ <span style=\"color: #008000\">0.7500</span>  │ <span style=\"color: #008000\">0.0039</span>  │ nan      │ <span style=\"color: #008000\">0.0084</span> │ <span style=\"color: #008000\">0.0400</span> │ <span style=\"color: #008000\">0.0047</span> │ 1.38e-03 │ 1.38e-03 │ 3.20e-05 │\n",
       "│ 4     │ 34   │ nan        │ <span style=\"color: #008000\">0.0183</span>   │ <span style=\"color: #800000\">0.2769</span>  │ <span style=\"color: #008000\">0.0094</span>  │ nan      │ <span style=\"color: #800000\">0.0070</span> │ <span style=\"color: #800000\">0.0400</span> │ <span style=\"color: #800000\">0.0038</span> │ 1.35e-03 │ 1.35e-03 │ 4.00e-05 │\n",
       "│ 5     │ 41   │ nan        │ <span style=\"color: #008000\">0.0270</span>   │ <span style=\"color: #800000\">0.3636</span>  │ <span style=\"color: #008000\">0.0140</span>  │ nan      │ <span style=\"color: #800000\">0.0071</span> │ <span style=\"color: #008000\">0.0476</span> │ <span style=\"color: #800000\">0.0038</span> │ 1.32e-03 │ 1.32e-03 │ 3.91e-05 │\n",
       "│ 6     │ 48   │ nan        │ <span style=\"color: #008000\">0.0367</span>   │ <span style=\"color: #800000\">0.4397</span>  │ <span style=\"color: #008000\">0.0191</span>  │ nan      │ <span style=\"color: #008000\">0.0134</span> │ <span style=\"color: #008000\">0.0970</span> │ <span style=\"color: #008000\">0.0072</span> │ 1.29e-03 │ 1.29e-03 │ 3.82e-05 │\n",
       "│ 7     │ 55   │ nan        │ <span style=\"color: #008000\">0.0551</span>   │ <span style=\"color: #800000\">0.3555</span>  │ <span style=\"color: #008000\">0.0299</span>  │ nan      │ <span style=\"color: #008000\">0.0181</span> │ <span style=\"color: #008000\">0.1370</span> │ <span style=\"color: #008000\">0.0097</span> │ 1.26e-03 │ 1.26e-03 │ 3.73e-05 │\n",
       "│ 8     │ 62   │ nan        │ <span style=\"color: #008000\">0.0920</span>   │ <span style=\"color: #800000\">0.4411</span>  │ <span style=\"color: #008000\">0.0513</span>  │ nan      │ <span style=\"color: #008000\">0.0219</span> │ <span style=\"color: #008000\">0.1742</span> │ <span style=\"color: #008000\">0.0117</span> │ 1.23e-03 │ 1.23e-03 │ 3.64e-05 │\n",
       "│ 9     │ 69   │ nan        │ <span style=\"color: #008000\">0.1125</span>   │ <span style=\"color: #800000\">0.4919</span>  │ <span style=\"color: #008000\">0.0635</span>  │ nan      │ <span style=\"color: #008000\">0.0292</span> │ <span style=\"color: #008000\">0.2260</span> │ <span style=\"color: #008000\">0.0156</span> │ 1.20e-03 │ 1.20e-03 │ 3.56e-05 │\n",
       "│ 10    │ 76   │ nan        │ <span style=\"color: #008000\">0.1287</span>   │ <span style=\"color: #800000\">0.5284</span>  │ <span style=\"color: #008000\">0.0733</span>  │ nan      │ <span style=\"color: #008000\">0.0351</span> │ <span style=\"color: #008000\">0.2500</span> │ <span style=\"color: #008000\">0.0189</span> │ 1.17e-03 │ 1.17e-03 │ 3.47e-05 │\n",
       "│ 11    │ 83   │ nan        │ <span style=\"color: #008000\">0.1579</span>   │ <span style=\"color: #800000\">0.5525</span>  │ <span style=\"color: #008000\">0.0921</span>  │ nan      │ <span style=\"color: #008000\">0.0427</span> │ <span style=\"color: #008000\">0.2679</span> │ <span style=\"color: #008000\">0.0232</span> │ 1.14e-03 │ 1.14e-03 │ 3.38e-05 │\n",
       "│ 12    │ 90   │ nan        │ <span style=\"color: #008000\">0.1842</span>   │ <span style=\"color: #800000\">0.5822</span>  │ <span style=\"color: #008000\">0.1094</span>  │ nan      │ <span style=\"color: #008000\">0.0477</span> │ <span style=\"color: #008000\">0.2905</span> │ <span style=\"color: #008000\">0.0260</span> │ 1.11e-03 │ 1.11e-03 │ 3.29e-05 │\n",
       "│ 13    │ 97   │ nan        │ <span style=\"color: #008000\">0.2135</span>   │ <span style=\"color: #800000\">0.6106</span>  │ <span style=\"color: #008000\">0.1294</span>  │ nan      │ <span style=\"color: #008000\">0.0571</span> │ <span style=\"color: #008000\">0.3102</span> │ <span style=\"color: #008000\">0.0315</span> │ 1.08e-03 │ 1.08e-03 │ 3.20e-05 │\n",
       "│ 14    │ 104  │ nan        │ <span style=\"color: #008000\">0.2407</span>   │ <span style=\"color: #800000\">0.6345</span>  │ <span style=\"color: #008000\">0.1486</span>  │ nan      │ <span style=\"color: #008000\">0.0587</span> │ <span style=\"color: #008000\">0.3237</span> │ <span style=\"color: #008000\">0.0323</span> │ 1.05e-03 │ 1.05e-03 │ 3.11e-05 │\n",
       "│ 15    │ 111  │ nan        │ <span style=\"color: #008000\">0.2661</span>   │ <span style=\"color: #800000\">0.6497</span>  │ <span style=\"color: #008000\">0.1673</span>  │ nan      │ <span style=\"color: #008000\">0.0687</span> │ <span style=\"color: #800000\">0.3237</span> │ <span style=\"color: #008000\">0.0384</span> │ 1.02e-03 │ 1.02e-03 │ 3.02e-05 │\n",
       "│ 16    │ 118  │ nan        │ <span style=\"color: #008000\">0.2888</span>   │ <span style=\"color: #800000\">0.6659</span>  │ <span style=\"color: #008000\">0.1843</span>  │ nan      │ <span style=\"color: #008000\">0.0757</span> │ <span style=\"color: #008000\">0.3333</span> │ <span style=\"color: #008000\">0.0427</span> │ 9.90e-04 │ 9.90e-04 │ 2.93e-05 │\n",
       "│ 17    │ 125  │ nan        │ <span style=\"color: #008000\">0.3145</span>   │ <span style=\"color: #800000\">0.6753</span>  │ <span style=\"color: #008000\">0.2050</span>  │ nan      │ <span style=\"color: #008000\">0.0781</span> │ <span style=\"color: #008000\">0.3430</span> │ <span style=\"color: #008000\">0.0441</span> │ 9.60e-04 │ 9.60e-04 │ 2.84e-05 │\n",
       "│ 18    │ 132  │ nan        │ <span style=\"color: #008000\">0.3287</span>   │ <span style=\"color: #800000\">0.6885</span>  │ <span style=\"color: #008000\">0.2159</span>  │ nan      │ <span style=\"color: #008000\">0.0879</span> │ <span style=\"color: #008000\">0.3492</span> │ <span style=\"color: #008000\">0.0503</span> │ 9.30e-04 │ 9.30e-04 │ 2.76e-05 │\n",
       "│ 19    │ 139  │ nan        │ <span style=\"color: #008000\">0.3474</span>   │ <span style=\"color: #800000\">0.6959</span>  │ <span style=\"color: #008000\">0.2315</span>  │ nan      │ <span style=\"color: #008000\">0.0940</span> │ <span style=\"color: #008000\">0.3563</span> │ <span style=\"color: #008000\">0.0541</span> │ 9.00e-04 │ 9.00e-04 │ 2.67e-05 │\n",
       "│ 20    │ 146  │ nan        │ <span style=\"color: #008000\">0.3695</span>   │ <span style=\"color: #800000\">0.7064</span>  │ <span style=\"color: #008000\">0.2502</span>  │ nan      │ <span style=\"color: #008000\">0.1009</span> │ <span style=\"color: #008000\">0.3566</span> │ <span style=\"color: #008000\">0.0587</span> │ 8.70e-04 │ 8.70e-04 │ 2.58e-05 │\n",
       "│ 21    │ 153  │ nan        │ <span style=\"color: #008000\">0.3863</span>   │ <span style=\"color: #800000\">0.7159</span>  │ <span style=\"color: #008000\">0.2645</span>  │ nan      │ <span style=\"color: #008000\">0.1078</span> │ <span style=\"color: #800000\">0.3463</span> │ <span style=\"color: #008000\">0.0638</span> │ 8.40e-04 │ 8.40e-04 │ 2.49e-05 │\n",
       "│ 22    │ 160  │ nan        │ <span style=\"color: #008000\">0.4032</span>   │ <span style=\"color: #800000\">0.7225</span>  │ <span style=\"color: #008000\">0.2796</span>  │ nan      │ <span style=\"color: #008000\">0.1115</span> │ <span style=\"color: #800000\">0.3510</span> │ <span style=\"color: #008000\">0.0663</span> │ 8.10e-04 │ 8.10e-04 │ 2.40e-05 │\n",
       "│ 23    │ 167  │ nan        │ <span style=\"color: #008000\">0.4207</span>   │ <span style=\"color: #800000\">0.7299</span>  │ <span style=\"color: #008000\">0.2955</span>  │ nan      │ <span style=\"color: #008000\">0.1140</span> │ <span style=\"color: #008000\">0.3583</span> │ <span style=\"color: #008000\">0.0678</span> │ 7.80e-04 │ 7.80e-04 │ 2.31e-05 │\n",
       "│ 24    │ 174  │ nan        │ <span style=\"color: #008000\">0.4347</span>   │ <span style=\"color: #800000\">0.7393</span>  │ <span style=\"color: #008000\">0.3078</span>  │ nan      │ <span style=\"color: #008000\">0.1223</span> │ <span style=\"color: #008000\">0.3625</span> │ <span style=\"color: #008000\">0.0735</span> │ 7.50e-04 │ 7.50e-04 │ 2.22e-05 │\n",
       "│ 25    │ 181  │ nan        │ <span style=\"color: #008000\">0.4526</span>   │ <span style=\"color: #800000\">0.7483</span>  │ <span style=\"color: #008000\">0.3244</span>  │ nan      │ <span style=\"color: #008000\">0.1257</span> │ <span style=\"color: #008000\">0.3656</span> │ <span style=\"color: #008000\">0.0759</span> │ 7.20e-04 │ 7.20e-04 │ 2.13e-05 │\n",
       "│ 26    │ 188  │ nan        │ <span style=\"color: #008000\">0.4686</span>   │ <span style=\"color: #008000\">0.7572</span>  │ <span style=\"color: #008000\">0.3393</span>  │ nan      │ <span style=\"color: #008000\">0.1323</span> │ <span style=\"color: #008000\">0.3694</span> │ <span style=\"color: #008000\">0.0806</span> │ 6.90e-04 │ 6.90e-04 │ 2.04e-05 │\n",
       "│ 27    │ 195  │ nan        │ <span style=\"color: #008000\">0.4832</span>   │ <span style=\"color: #008000\">0.7646</span>  │ <span style=\"color: #008000\">0.3532</span>  │ nan      │ <span style=\"color: #008000\">0.1353</span> │ <span style=\"color: #008000\">0.3742</span> │ <span style=\"color: #008000\">0.0826</span> │ 6.60e-04 │ 6.60e-04 │ 1.96e-05 │\n",
       "│ 28    │ 202  │ nan        │ <span style=\"color: #008000\">0.4979</span>   │ <span style=\"color: #008000\">0.7731</span>  │ <span style=\"color: #008000\">0.3672</span>  │ nan      │ <span style=\"color: #008000\">0.1401</span> │ <span style=\"color: #800000\">0.3741</span> │ <span style=\"color: #008000\">0.0862</span> │ 6.30e-04 │ 6.30e-04 │ 1.87e-05 │\n",
       "│ 29    │ 209  │ nan        │ <span style=\"color: #008000\">0.5123</span>   │ <span style=\"color: #008000\">0.7781</span>  │ <span style=\"color: #008000\">0.3819</span>  │ nan      │ <span style=\"color: #800000\">0.1397</span> │ <span style=\"color: #008000\">0.3754</span> │ <span style=\"color: #800000\">0.0858</span> │ 6.00e-04 │ 6.00e-04 │ 1.78e-05 │\n",
       "│ 30    │ 216  │ nan        │ <span style=\"color: #008000\">0.5240</span>   │ <span style=\"color: #008000\">0.7849</span>  │ <span style=\"color: #008000\">0.3933</span>  │ nan      │ <span style=\"color: #008000\">0.1436</span> │ <span style=\"color: #800000\">0.3747</span> │ <span style=\"color: #008000\">0.0888</span> │ 5.70e-04 │ 5.70e-04 │ 1.69e-05 │\n",
       "│ 31    │ 223  │ nan        │ <span style=\"color: #008000\">0.5359</span>   │ <span style=\"color: #008000\">0.7910</span>  │ <span style=\"color: #008000\">0.4052</span>  │ nan      │ <span style=\"color: #008000\">0.1450</span> │ <span style=\"color: #008000\">0.3785</span> │ <span style=\"color: #008000\">0.0897</span> │ 5.40e-04 │ 5.40e-04 │ 1.60e-05 │\n",
       "│ 32    │ 230  │ nan        │ <span style=\"color: #008000\">0.5477</span>   │ <span style=\"color: #008000\">0.7977</span>  │ <span style=\"color: #008000\">0.4170</span>  │ nan      │ <span style=\"color: #008000\">0.1476</span> │ <span style=\"color: #008000\">0.3801</span> │ <span style=\"color: #008000\">0.0916</span> │ 5.10e-04 │ 5.10e-04 │ 1.51e-05 │\n",
       "│ 33    │ 237  │ nan        │ <span style=\"color: #008000\">0.5584</span>   │ <span style=\"color: #008000\">0.8041</span>  │ <span style=\"color: #008000\">0.4277</span>  │ nan      │ <span style=\"color: #008000\">0.1500</span> │ <span style=\"color: #800000\">0.3800</span> │ <span style=\"color: #008000\">0.0934</span> │ 4.80e-04 │ 4.80e-04 │ 1.42e-05 │\n",
       "│ 34    │ 244  │ nan        │ <span style=\"color: #008000\">0.5701</span>   │ <span style=\"color: #008000\">0.8096</span>  │ <span style=\"color: #008000\">0.4400</span>  │ nan      │ <span style=\"color: #008000\">0.1523</span> │ <span style=\"color: #008000\">0.3825</span> │ <span style=\"color: #008000\">0.0951</span> │ 4.50e-04 │ 4.50e-04 │ 1.33e-05 │\n",
       "│ 35    │ 251  │ nan        │ <span style=\"color: #008000\">0.5799</span>   │ <span style=\"color: #008000\">0.8159</span>  │ <span style=\"color: #008000\">0.4498</span>  │ nan      │ <span style=\"color: #008000\">0.1552</span> │ <span style=\"color: #008000\">0.3828</span> │ <span style=\"color: #008000\">0.0973</span> │ 4.20e-04 │ 4.20e-04 │ 1.24e-05 │\n",
       "│ 36    │ 258  │ nan        │ <span style=\"color: #008000\">0.5903</span>   │ <span style=\"color: #008000\">0.8191</span>  │ <span style=\"color: #008000\">0.4614</span>  │ nan      │ <span style=\"color: #008000\">0.1574</span> │ <span style=\"color: #008000\">0.3846</span> │ <span style=\"color: #008000\">0.0989</span> │ 3.90e-04 │ 3.90e-04 │ 1.16e-05 │\n",
       "│ 37    │ 265  │ nan        │ <span style=\"color: #008000\">0.5990</span>   │ <span style=\"color: #008000\">0.8250</span>  │ <span style=\"color: #008000\">0.4702</span>  │ nan      │ <span style=\"color: #008000\">0.1600</span> │ <span style=\"color: #008000\">0.3858</span> │ <span style=\"color: #008000\">0.1009</span> │ 3.60e-04 │ 3.60e-04 │ 1.07e-05 │\n",
       "│ 38    │ 272  │ nan        │ <span style=\"color: #008000\">0.6093</span>   │ <span style=\"color: #008000\">0.8296</span>  │ <span style=\"color: #008000\">0.4815</span>  │ nan      │ <span style=\"color: #008000\">0.1618</span> │ <span style=\"color: #008000\">0.3867</span> │ <span style=\"color: #008000\">0.1023</span> │ 3.30e-04 │ 3.30e-04 │ 9.78e-06 │\n",
       "│ 39    │ 279  │ nan        │ <span style=\"color: #008000\">0.6178</span>   │ <span style=\"color: #008000\">0.8338</span>  │ <span style=\"color: #008000\">0.4906</span>  │ nan      │ <span style=\"color: #008000\">0.1632</span> │ <span style=\"color: #008000\">0.3878</span> │ <span style=\"color: #008000\">0.1034</span> │ 3.00e-04 │ 3.00e-04 │ 8.89e-06 │\n",
       "│ 40    │ 286  │ nan        │ <span style=\"color: #008000\">0.6263</span>   │ <span style=\"color: #008000\">0.8380</span>  │ <span style=\"color: #008000\">0.5000</span>  │ nan      │ <span style=\"color: #008000\">0.1649</span> │ <span style=\"color: #008000\">0.3887</span> │ <span style=\"color: #008000\">0.1046</span> │ 2.70e-04 │ 2.70e-04 │ 8.00e-06 │\n",
       "│ 41    │ 293  │ nan        │ <span style=\"color: #008000\">0.6350</span>   │ <span style=\"color: #008000\">0.8424</span>  │ <span style=\"color: #008000\">0.5096</span>  │ nan      │ <span style=\"color: #008000\">0.1664</span> │ <span style=\"color: #008000\">0.3895</span> │ <span style=\"color: #008000\">0.1058</span> │ 2.40e-04 │ 2.40e-04 │ 7.11e-06 │\n",
       "│ 42    │ 300  │ nan        │ <span style=\"color: #008000\">0.6428</span>   │ <span style=\"color: #008000\">0.8465</span>  │ <span style=\"color: #008000\">0.5181</span>  │ nan      │ <span style=\"color: #008000\">0.1674</span> │ <span style=\"color: #008000\">0.3897</span> │ <span style=\"color: #008000\">0.1066</span> │ 2.10e-04 │ 2.10e-04 │ 6.22e-06 │\n",
       "│ 43    │ 307  │ nan        │ <span style=\"color: #008000\">0.6503</span>   │ <span style=\"color: #008000\">0.8503</span>  │ <span style=\"color: #008000\">0.5265</span>  │ nan      │ <span style=\"color: #008000\">0.1694</span> │ <span style=\"color: #800000\">0.3895</span> │ <span style=\"color: #008000\">0.1082</span> │ 1.80e-04 │ 1.80e-04 │ 5.33e-06 │\n",
       "│ 44    │ 314  │ nan        │ <span style=\"color: #008000\">0.6574</span>   │ <span style=\"color: #008000\">0.8529</span>  │ <span style=\"color: #008000\">0.5348</span>  │ nan      │ <span style=\"color: #008000\">0.1705</span> │ <span style=\"color: #008000\">0.3899</span> │ <span style=\"color: #008000\">0.1091</span> │ 1.50e-04 │ 1.50e-04 │ 4.44e-06 │\n",
       "│ 45    │ 321  │ nan        │ <span style=\"color: #008000\">0.6642</span>   │ <span style=\"color: #008000\">0.8566</span>  │ <span style=\"color: #008000\">0.5424</span>  │ nan      │ <span style=\"color: #008000\">0.1714</span> │ <span style=\"color: #008000\">0.3905</span> │ <span style=\"color: #008000\">0.1098</span> │ 1.20e-04 │ 1.20e-04 │ 3.56e-06 │\n",
       "│ 46    │ 328  │ nan        │ <span style=\"color: #008000\">0.6705</span>   │ <span style=\"color: #008000\">0.8596</span>  │ <span style=\"color: #008000\">0.5496</span>  │ nan      │ <span style=\"color: #008000\">0.1726</span> │ <span style=\"color: #008000\">0.3907</span> │ <span style=\"color: #008000\">0.1107</span> │ 9.00e-05 │ 9.00e-05 │ 2.67e-06 │\n",
       "│ 47    │ 335  │ nan        │ <span style=\"color: #008000\">0.6767</span>   │ <span style=\"color: #008000\">0.8624</span>  │ <span style=\"color: #008000\">0.5568</span>  │ nan      │ <span style=\"color: #008000\">0.1737</span> │ <span style=\"color: #008000\">0.3909</span> │ <span style=\"color: #008000\">0.1117</span> │ 6.00e-05 │ 6.00e-05 │ 1.78e-06 │\n",
       "│ 48    │ 342  │ nan        │ <span style=\"color: #008000\">0.6828</span>   │ <span style=\"color: #008000\">0.8650</span>  │ <span style=\"color: #008000\">0.5640</span>  │ nan      │ <span style=\"color: #008000\">0.1747</span> │ <span style=\"color: #008000\">0.3909</span> │ <span style=\"color: #008000\">0.1125</span> │ 3.00e-05 │ 3.00e-05 │ 8.89e-07 │\n",
       "│ 49    │ 349  │ nan        │ <span style=\"color: #008000\">0.6884</span>   │ <span style=\"color: #008000\">0.8674</span>  │ <span style=\"color: #008000\">0.5706</span>  │ nan      │ <span style=\"color: #008000\">0.1757</span> │ <span style=\"color: #008000\">0.3909</span> │ <span style=\"color: #008000\">0.1133</span> │ 0.00e+00 │ 0.00e+00 │ 0.00e+00 │\n",
       "└───────┴──────┴────────────┴──────────┴─────────┴─────────┴──────────┴────────┴────────┴────────┴──────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich_logger.table_printer.RichTablePrinter at 0x7f3045ed8c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/pyner/pyner')\n",
    "\n",
    "from pyner import NER, Vocabulary\n",
    "from pyner.datasets import BRATDataset\n",
    "import string\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rich_logger import RichTableLogger\n",
    "\n",
    "# bert_name = \"bert-base-german-cased\"\n",
    "bert_name = \"bert-base-multilingual-cased\"\n",
    "model = NER(\n",
    "    seed=42,\n",
    "    preprocessor=dict(\n",
    "        module=\"preprocessor\",\n",
    "        bert_name=bert_name, # transformer name\n",
    "        sentence_split_regex=r\"((?:\\s*\\n)+\\s*|(?:(?<=[a-z0-9)]\\.)\\s+))(?=[A-Z-])\", # regex to use to split sentences (must not contain consuming patterns)\n",
    "        sentence_balance_chars=('()',), # try to avoid splitting between parentheses\n",
    "        sentence_entity_overlap=\"split\", # raise when an entity spans more than one sentence, or use \"split\" to split entities in 2 when this happens\n",
    "        word_regex='[\\\\w\\']+|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', # regex to use to extract words (will be aligned with bert tokens), leave to None to use wordpieces as is\n",
    "        substitutions=( # Apply these regex substitutions on sentences before tokenizing\n",
    "            (r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"), # insert a space before punctuations\n",
    "            (r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"), # insert a space after punctuations\n",
    "            #(\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"), # insert a space between letters and numbers\n",
    "            #(\"(?<=[0-9])(?=[A-Za-z])\", r\" \"), # insert a space between numbers and letters\n",
    "        ),\n",
    "        max_tokens=512,         # Maximum number of tokens in a sentence (will split if more than this number)\n",
    "                                # Must be equal to or lower than the max number of tokens in the Bert model\n",
    "        large_sentences=\"equal-split\", # for these large sentences, split them in equal sub sentences < max_tokens tokens \n",
    "        empty_entities=\"raise\", # when an entity cannot be mapped to any word, \"raise\" or \"drop\"\n",
    "        vocabularies=torch.nn.ModuleDict({ # vocabularies to use, call .train() before initializing to fill/complete them automatically from training data\n",
    "            \"char\": Vocabulary(string.punctuation + string.ascii_letters + string.digits, with_unk=True, with_pad=True),\n",
    "            \"label\": Vocabulary(with_unk=True, with_pad=False),\n",
    "        }).train(),\n",
    "    ),\n",
    "\n",
    "    # Word encore parameters\n",
    "    word_encoders=[\n",
    "        dict(\n",
    "            module=\"char_cnn\",\n",
    "            n_chars=None, # automatically inferred from data\n",
    "            in_channels=8,\n",
    "            out_channels=50,\n",
    "            kernel_sizes=(3, 4, 5),\n",
    "        ),\n",
    "        dict(\n",
    "            module=\"bert\",\n",
    "            path=bert_name,\n",
    "            n_layers=4,\n",
    "            freeze_n_layers=0, # unfreeze all\n",
    "            dropout_p=0.1,\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    # Decoder parameters\n",
    "    decoder=dict(\n",
    "        module=\"exhaustive_biaffine_ner\",\n",
    "        dim=192,\n",
    "        label_dim=64,\n",
    "        n_labels=None, # automatically inferred from data\n",
    "        dropout_p=0.,\n",
    "        use_batch_norm=False,\n",
    "        contextualizer=dict(\n",
    "            module=\"lstm\",\n",
    "            # use gate = False for better performance but slower convergence (needs ~50 epochs)\n",
    "            gate=dict(\n",
    "                module=\"sigmoid_gate\",\n",
    "                ln_mode=\"pre\",\n",
    "                init_value=0,\n",
    "                proj=False,\n",
    "                dim=192,\n",
    "            ),\n",
    "            input_size=768 + 150,\n",
    "            hidden_size=192,\n",
    "            num_layers=4,\n",
    "            dropout_p=0.,\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    # Initialize last classifying layer bias with log frequencies from labels in data\n",
    "    init_labels_bias=True,\n",
    "\n",
    "    batch_size=16,\n",
    "    \n",
    "    # Use learning rate schedules (linearly decay with warmup)\n",
    "    use_lr_schedules=True,\n",
    "    warmup_rate=0.1,\n",
    "\n",
    "    gradient_clip_val=5.,\n",
    "    \n",
    "    # Learning rates\n",
    "    main_lr=1.5e-3,\n",
    "    top_lr=1.5e-3,\n",
    "    bert_lr=4e-5,\n",
    "    \n",
    "    # Optimizer, can be class or str\n",
    "    optimizer_cls=\"transformers.AdamW\",\n",
    ").train()\n",
    "\n",
    "flt_format = (5, \"{:.4f}\".format)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    progress_bar_refresh_rate=False,\n",
    "    move_metrics_to_cpu=True,\n",
    "    logger=[\n",
    "        #        pl.loggers.TestTubeLogger(\"path/to/logs\", name=\"my_experiment\"),\n",
    "        RichTableLogger(key=\"epoch\", fields={\n",
    "            \"epoch\": {},\n",
    "            \"step\": {},\n",
    "            \"train_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\"},\n",
    "            \"train_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"train_f1\"},\n",
    "            \"train_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"train_p\"},\n",
    "            \"train_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"train_r\"},\n",
    "\n",
    "            \"val_loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.4f}\"},\n",
    "            \"val_f1\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"val_f1\"},\n",
    "            \"val_precision\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"val_p\"},\n",
    "            \"val_recall\": {\"goal\": \"higher_is_better\", \"format\": \"{:.4f}\", \"name\": \"val_r\"},\n",
    "\n",
    "            \"main_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"top_lr\": {\"format\": \"{:.2e}\"},\n",
    "            \"bert_lr\": {\"format\": \"{:.2e}\"},\n",
    "        }\n",
    "                       \n",
    "       ),\n",
    "    ],\n",
    "    max_epochs=50)\n",
    "\n",
    "# N2C2 PATH:\n",
    "# /home/ytaille/data/resources/n2c2/brat_files/train\n",
    "\n",
    "# QUAERO PATH:\n",
    "# /home/ytaille/data/resources/quaero/corpus/train/MEDLINE\n",
    "\n",
    "# MANTRA PATH:\n",
    "# /home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\n",
    "# /home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\n",
    "\n",
    "# CONLL PATH:\n",
    "# /home/ytaille/data/resources/conll/brat/eng/train\n",
    "\n",
    "\n",
    "dataset = BRATDataset(\n",
    "    train=[\n",
    "        \"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/EMEA_ec22-cui-best_man\",\n",
    "    ],\n",
    "#     test=\"/home/ytaille/data/resources/quaero/corpus/test/MEDLINE\",#\"path/to/brat/test\",    # None for training only, test directory otherwise\n",
    "    val=\"/home/ytaille/data/resources/mantra/Mantra-GSC_new_ann/French/Medline_EN_FR_ec22-cui-best_man\",  # first 20% doc will be for validation\n",
    "    seed=42,  # don't shuffle before splitting4\n",
    "    ger_num=1000,\n",
    ")\n",
    "\n",
    "trainer.fit(model, dataset)\n",
    "model.save_pretrained(\"conll_ner.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on quaero and test on Mantra\n",
    "# deft, quaero\n",
    "\n",
    "# compte rendus anglais étiquetés en concepts style UMLS (procédure, ANAT), puis utiliser corpus francais\n",
    "\n",
    "# Look for multilingual corpora \n",
    "\n",
    "# See tradeof between proportion of english corpus and target language -> at what proportion of target language are the results comparable with full target language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on CONLL for Multilingual transfer learning -> goal is for Aurélie Névéol to apply our system on Merlot if it works\n",
    "\n",
    "# https://www.clips.uantwerpen.be/conll2002/ner/ \n",
    "# https://www.clips.uantwerpen.be/conll2003/ner/ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
